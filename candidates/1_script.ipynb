{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** GENERATED PIPELINE ***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "train_dataset = pd.read_csv(\"./train_set.csv\", encoding=\"UTF-8\", delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TRAIN-TEST SPLIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "def split_dataset(dataset, train_size=0.85, random_state=1024):\n",
    "    train_dataset, test_dataset = train_test_split(dataset, train_size=train_size, random_state=random_state)\n",
    "    return train_dataset, test_dataset\t\n",
    "train_dataset, test_dataset = split_dataset(train_dataset)\n",
    "train_dataset, validation_dataset = split_dataset(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SUBSAMPLE<br>\n",
    "If the number of rows of train_dataset is larger than sample_size, sample rows to sample_size for speedup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib.sample_dataset import sample_dataset\n",
    "train_dataset = sample_dataset(\n",
    "    dataframe=train_dataset,\n",
    "    sample_size=100000,\n",
    "    target_columns=['Article_type'],\n",
    "    task_type='classification'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DROP IGNORED COLUMNS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ignore_columns = ['Article_ID']\n",
    "train_dataset = train_dataset.drop(ignore_columns, axis=1, errors=\"ignore\")\n",
    "validation_dataset = validation_dataset.drop(ignore_columns, axis=1, errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = validation_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPROCESSING-1<br>\n",
    "Component: Preprocess:TextPreprocessing<br>\n",
    "Efficient Cause: Preprocess:TextPreprocessing is required in this pipeline since the dataset has ['feature:str_text_presence']. The relevant features are: ['Article_content'].<br>\n",
    "Purpose: Preprocess and normalize text.<br>\n",
    "Form:<br>\n",
    "  Input: array of strings<br>\n",
    "  Key hyperparameters used: None<br>\n",
    "Alternatives: Although  can also be used for this dataset, Preprocess:TextPreprocessing is used because it has more  than .<br>\n",
    "Order: Preprocess:TextPreprocessing should be applied  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "TEXT_COLUMNS = ['Article_content']\n",
    "def process_text(__dataset):\n",
    "    for _col in TEXT_COLUMNS:\n",
    "        process_text = [t.lower() for t in __dataset[_col]]\n",
    "        # strip all punctuation\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        process_text = [t.translate(table) for t in process_text]\n",
    "        # convert all numbers in text to 'num'\n",
    "        process_text = [re.sub(r'\\d+', 'num', t) for t in process_text]\n",
    "        __dataset[_col] = process_text\n",
    "    return __dataset\n",
    "train_dataset = process_text(train_dataset)\n",
    "test_dataset = process_text(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DETACH TARGET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TARGET_COLUMNS = ['Article_type']\n",
    "feature_train = train_dataset.drop(TARGET_COLUMNS, axis=1)\n",
    "target_train = train_dataset[TARGET_COLUMNS].copy()\n",
    "feature_test = test_dataset.drop(TARGET_COLUMNS, axis=1)\n",
    "target_test = test_dataset[TARGET_COLUMNS].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREPROCESSING-2<br>\n",
    "Component: Preprocess:TfidfVectorizer<br>\n",
    "Efficient Cause: Preprocess:TfidfVectorizer is required in this pipeline since the dataset has ['feature:str_text_presence']. The relevant features are: ['Article_content'].<br>\n",
    "Purpose: Convert a collection of raw documents to a matrix of TF-IDF features.<br>\n",
    "Form:<br>\n",
    "  Input: raw_documents<br>\n",
    "  Key hyperparameters used: <br>\n",
    "\t \"max_features: int, default=None\" :: If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus. This parameter is ignored if vocabulary is not None.<br>\n",
    "Alternatives: Although  can also be used for this dataset, Preprocess:TfidfVectorizer is used because it has more  than .<br>\n",
    "Order: Preprocess:TfidfVectorizer should be applied  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "TEXT_COLUMNS = ['Article_content']\n",
    "temp_train_data = feature_train[TEXT_COLUMNS]\n",
    "temp_test_data = feature_test[TEXT_COLUMNS]\n",
    "# Make the entire dataframe sparse to avoid it converting into a dense matrix.\n",
    "feature_train = feature_train.drop(TEXT_COLUMNS, axis=1).astype(pd.SparseDtype('float64', 0))\n",
    "feature_test = feature_test.drop(TEXT_COLUMNS, axis=1).astype(pd.SparseDtype('float64', 0))\n",
    "for _col in TEXT_COLUMNS:\n",
    "    tfidfvectorizer = TfidfVectorizer(max_features=3000)\n",
    "    vector_train = tfidfvectorizer.fit_transform(temp_train_data[_col])\n",
    "    feature_names = ['_'.join([_col, name]) for name in tfidfvectorizer.get_feature_names_out()]\n",
    "    vector_train = pd.DataFrame.sparse.from_spmatrix(vector_train, columns=feature_names, index=temp_train_data.index)\n",
    "    feature_train = pd.concat([feature_train, vector_train], axis=1)\n",
    "    vector_test = tfidfvectorizer.transform(temp_test_data[_col])\n",
    "    vector_test = pd.DataFrame.sparse.from_spmatrix(vector_test, columns=feature_names, index=temp_test_data.index)\n",
    "    feature_test = pd.concat([feature_test, vector_test], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "model = GaussianNB()\n",
    "model.fit(feature_train, target_train.values.ravel())\n",
    "y_pred = model.predict_proba(feature_test)\n",
    "# POST PROCESSING\n",
    "if np.shape(y_pred)[1] == 2:\n",
    "    y_pred = y_pred[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "__roc_auc = roc_auc_score(target_test, y_pred)\n",
    "print('RESULT: ROC AUC:', str(__roc_auc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
